{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89ab14f5",
   "metadata": {},
   "source": [
    "# Multimodal RAG Pipeline with LangChain + Ollama + Unstructured\n",
    "\n",
    "## Complete RAG System Using Open-Source Tools\n",
    "\n",
    "This notebook implements a production-ready multimodal RAG pipeline using:\n",
    "\n",
    "### ğŸ› ï¸ Tech Stack:\n",
    "- **LangChain**: Orchestration framework\n",
    "- **Ollama**: Local LLM inference (llama3.1, mistral, etc.)\n",
    "- **Unstructured**: Advanced document parsing (PDFs, tables, images)\n",
    "- **ChromaDB**: Vector database\n",
    "- **MultiVectorRetriever**: Handle summaries + raw documents\n",
    "\n",
    "### ğŸ“¦ Features:\n",
    "âœ… Extract text, tables, and images from PDFs\n",
    "âœ… Summarize each element for better retrieval\n",
    "âœ… Store summaries in vector DB, raw content separately\n",
    "âœ… Retrieve relevant context and generate grounded answers\n",
    "âœ… 100% local and open-source\n",
    "âœ… No API costs\n",
    "âœ… **ğŸ†• Process multiple PDFs with source tracking**\n",
    "âœ… **ğŸ†• Query specific documents or across all PDFs**\n",
    "âœ… **ğŸ†• Compare information between documents**\n",
    "âœ… **ğŸ†• Incrementally add new PDFs to existing database**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e20bfd",
   "metadata": {},
   "source": [
    "## Step 0: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "35beacdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorboard 2.15.1 requires protobuf<4.24,>=3.19.6, but you have protobuf 6.32.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install all required packages\n",
    "%pip install -q langchain langchain-community langchain-ollama\n",
    "%pip install -q chromadb\n",
    "%pip install -q \"unstructured[all-docs]\" python-magic\n",
    "%pip install -q pillow pdf2image pdfminer-six\n",
    "%pip install -q pandas openpyxl tabulate\n",
    "%pip install -q pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "45cc0f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Using cached numpy-2.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.7.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Using cached scipy-1.16.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas)\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Using cached numpy-2.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "Using cached pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n",
      "Using cached scikit_learn-1.7.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.5 MB)\n",
      "Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached scipy-1.16.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.7 MB)\n",
      "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, threadpoolctl, six, numpy, joblib, scipy, python-dateutil, scikit-learn, pandas\n",
      "\u001b[2K  Attempting uninstall: pytz\n",
      "\u001b[2K    Found existing installation: pytz 2025.2\n",
      "\u001b[2K    Uninstalling pytz-2025.2:\n",
      "\u001b[2K      Successfully uninstalled pytz-2025.2\n",
      "\u001b[2K  Attempting uninstall: tzdataâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 0/10\u001b[0m [pytz]\n",
      "\u001b[2K    Found existing installation: tzdata 2025.20m \u001b[32m 0/10\u001b[0m [pytz]\n",
      "\u001b[2K    Uninstalling tzdata-2025.2:â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 0/10\u001b[0m [pytz]\n",
      "\u001b[2K      Successfully uninstalled tzdata-2025.2\u001b[0m \u001b[32m 0/10\u001b[0m [pytz]\n",
      "\u001b[2K  Attempting uninstall: threadpoolctlâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 1/10\u001b[0m [tzdata]\n",
      "\u001b[2K    Found existing installation: threadpoolctl 3.6.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 2/10\u001b[0m [threadpoolctl]\n",
      "\u001b[2K    Uninstalling threadpoolctl-3.6.0:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 2/10\u001b[0m [threadpoolctl]\n",
      "\u001b[2K      Successfully uninstalled threadpoolctl-3.6.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 2/10\u001b[0m [threadpoolctl]\n",
      "\u001b[2K  Attempting uninstall: six[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 2/10\u001b[0m [threadpoolctl]\n",
      "\u001b[2K    Found existing installation: six 1.17.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 2/10\u001b[0m [threadpoolctl]\n",
      "\u001b[2K    Uninstalling six-1.17.0:90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 2/10\u001b[0m [threadpoolctl]\n",
      "\u001b[2K      Successfully uninstalled six-1.17.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 3/10\u001b[0m [six]olctl]\n",
      "\u001b[2K  Attempting uninstall: numpym\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 3/10\u001b[0m [six]\n",
      "\u001b[2K    Found existing installation: numpy 2.2.6â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 3/10\u001b[0m [six]\n",
      "\u001b[2K    Uninstalling numpy-2.2.6:m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 3/10\u001b[0m [six]\n",
      "\u001b[2K      Successfully uninstalled numpy-2.2.6â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 3/10\u001b[0m [six]\n",
      "\u001b[2K  Attempting uninstall: joblib90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 4/10\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: joblib 1.5.2â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 4/10\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling joblib-1.5.2:\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 4/10\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled joblib-1.5.2â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 4/10\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: scipy[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 5/10\u001b[0m [joblib]\n",
      "\u001b[2K    Found existing installation: scipy 1.16.2â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 5/10\u001b[0m [joblib]\n",
      "\u001b[2K    Uninstalling scipy-1.16.2:90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 5/10\u001b[0m [joblib]\n",
      "\u001b[2K      Successfully uninstalled scipy-1.16.2â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 5/10\u001b[0m [joblib]\n",
      "\u001b[2K  Attempting uninstall: python-dateutil0mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 6/10\u001b[0m [scipy]\n",
      "\u001b[2K    Found existing installation: python-dateutil 2.9.0.post0â”â”\u001b[0m \u001b[32m 6/10\u001b[0m [scipy]\n",
      "\u001b[2K    Uninstalling python-dateutil-2.9.0.post0:0mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 6/10\u001b[0m [scipy]\n",
      "\u001b[2K      Successfully uninstalled python-dateutil-2.9.0.post0â”â”â”â”\u001b[0m \u001b[32m 6/10\u001b[0m [scipy]\n",
      "\u001b[2K  Attempting uninstall: scikit-learn\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 7/10\u001b[0m [python-dateutil]\n",
      "\u001b[2K    Found existing installation: scikit-learn 1.7.2â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 7/10\u001b[0m [python-dateutil]\n",
      "\u001b[2K    Uninstalling scikit-learn-1.7.2:â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”\u001b[0m \u001b[32m 8/10\u001b[0m [scikit-learn]\n",
      "\u001b[2K      Successfully uninstalled scikit-learn-1.7.2m\u001b[90mâ”â”â”â”â”â”â”\u001b[0m \u001b[32m 8/10\u001b[0m [scikit-learn]\n",
      "\u001b[2K  Attempting uninstall: pandasâ”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”\u001b[0m \u001b[32m 8/10\u001b[0m [scikit-learn]\n",
      "\u001b[2K    Found existing installation: pandas 2.3.3â•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”\u001b[0m \u001b[32m 8/10\u001b[0m [scikit-learn]\n",
      "\u001b[2K    Uninstalling pandas-2.3.3:â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”\u001b[0m \u001b[32m 9/10\u001b[0m [pandas]n]\n",
      "\u001b[2K      Successfully uninstalled pandas-2.3.3m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”\u001b[0m \u001b[32m 9/10\u001b[0m [pandas]\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10/10\u001b[0m [pandas] 9/10\u001b[0m [pandas]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.3 which is incompatible.\n",
      "tensorboard 2.15.1 requires protobuf<4.24,>=3.19.6, but you have protobuf 6.32.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed joblib-1.5.2 numpy-2.3.3 pandas-2.3.3 python-dateutil-2.9.0.post0 pytz-2025.2 scikit-learn-1.7.2 scipy-1.16.2 six-1.17.0 threadpoolctl-3.6.0 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --force-reinstall numpy pandas scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f23968",
   "metadata": {},
   "source": [
    "## ğŸš€ Quick Start: Multi-PDF Processing\n",
    "\n",
    "### Single PDF Mode\n",
    "```python\n",
    "pdf_path = \"paper.pdf\"\n",
    "use_batch_mode = False\n",
    "```\n",
    "\n",
    "### Multi-PDF Mode (Recommended)\n",
    "```python\n",
    "pdf_paths = [\n",
    "    \"paper1.pdf\",\n",
    "    \"paper2.pdf\",\n",
    "    \"research_doc.pdf\"\n",
    "]\n",
    "use_batch_mode = True\n",
    "```\n",
    "\n",
    "### Key Features:\n",
    "- **Batch Processing**: Process multiple PDFs in one go\n",
    "- **Source Tracking**: Every document knows which PDF it came from\n",
    "- **Filtered Queries**: Query specific documents or all at once\n",
    "- **Incremental Updates**: Add new PDFs without reprocessing everything\n",
    "- **Document Comparison**: Compare information across sources\n",
    "\n",
    "See Step 4 to configure your PDF paths!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b739c0",
   "metadata": {},
   "source": [
    "## Prerequisites: Setup Ollama\n",
    "\n",
    "### 1. Install Ollama\n",
    "Download from: **https://ollama.ai/download**\n",
    "\n",
    "### 2. Start Ollama Server\n",
    "```bash\n",
    "ollama serve\n",
    "```\n",
    "\n",
    "### 3. Pull Required Models\n",
    "```bash\n",
    "# For text generation and summarization\n",
    "ollama pull gemma3:4b\n",
    "\n",
    "# For embeddings (lighter model)\n",
    "ollama pull nomic-embed-text\n",
    "\n",
    "# Optional: For vision tasks\n",
    "ollama pull llama3.2-vision\n",
    "```\n",
    "\n",
    "### 4. Verify Installation\n",
    "```bash\n",
    "ollama list\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d08062",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries and Test Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14563d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All imports successful!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.schema.document import Document\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Unstructured imports\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.partition.auto import partition\n",
    "\n",
    "# Other imports\n",
    "import pandas as pd\n",
    "from IPython.display import Image as IPImage, display, HTML\n",
    "import requests\n",
    "\n",
    "print(\"âœ“ All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54c5bbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ— Could not connect to Ollama: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/tags (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x733791c40890>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "\n",
      "Make sure to:\n",
      "  1. Install Ollama from https://ollama.ai/download\n",
      "  2. Run: ollama serve\n",
      "  3. Pull models: ollama pull llama3.1:8b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Ollama connection\n",
    "def test_ollama():\n",
    "    \"\"\"Test Ollama connection and list available models.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            models = response.json().get('models', [])\n",
    "            print(\"âœ“ Ollama is running!\")\n",
    "            print(f\"âœ“ Found {len(models)} models:\\n\")\n",
    "            for model in models:\n",
    "                print(f\"  â€¢ {model['name']}\")\n",
    "                print(f\"    Size: {model.get('size', 0) / 1e9:.2f} GB\\n\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"âœ— Ollama returned status: {response.status_code}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Could not connect to Ollama: {e}\")\n",
    "        print(\"\\nMake sure to:\")\n",
    "        print(\"  1. Install Ollama from https://ollama.ai/download\")\n",
    "        print(\"  2. Run: ollama serve\")\n",
    "        print(\"  3. Pull models: ollama pull llama3.1:8b\")\n",
    "        return False\n",
    "\n",
    "test_ollama()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bbba9b",
   "metadata": {},
   "source": [
    "## Step 2: Initialize LangChain Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5c02fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ LLM initialized: gemma3:4b\n",
      "âœ“ Embeddings initialized: embeddinggemma:300m\n",
      "\n",
      "You can change models by modifying the model parameter\n"
     ]
    }
   ],
   "source": [
    "# Initialize Ollama models\n",
    "llm = ChatOllama(\n",
    "    model=\"gemma3:4b\",\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"embeddinggemma:300m\",\n",
    ")\n",
    "\n",
    "print(\"âœ“ LLM initialized: gemma3:4b\")\n",
    "print(\"âœ“ Embeddings initialized: embeddinggemma:300m\")\n",
    "print(\"\\nYou can change models by modifying the model parameter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79692c9",
   "metadata": {},
   "source": [
    "## Step 3: Setup Vector Store and MultiVectorRetriever\n",
    "\n",
    "The MultiVectorRetriever allows us to:\n",
    "- Store **summaries** in the vector database (for efficient retrieval)\n",
    "- Store **raw content** separately (for context to the LLM)\n",
    "- Retrieve using summaries but return full content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9edf1650",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3195/547392683.py:2: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Vector store initialized (ChromaDB)\n",
      "âœ“ Document store initialized (In-Memory)\n",
      "âœ“ MultiVectorRetriever ready\n"
     ]
    }
   ],
   "source": [
    "# Initialize vector store\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"nasa_multimodal_rag\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\"\n",
    ")\n",
    "\n",
    "# Initialize document store for raw content\n",
    "docstore = InMemoryStore()\n",
    "\n",
    "# Setup ID key\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# Create MultiVectorRetriever\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=docstore,\n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "print(\"âœ“ Vector store initialized (ChromaDB)\")\n",
    "print(\"âœ“ Document store initialized (In-Memory)\")\n",
    "print(\"âœ“ MultiVectorRetriever ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e97478",
   "metadata": {},
   "source": [
    "## List the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2da31995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15 PDF files:\n",
      "  - pdfs/PMC11500582.pdf\n",
      "  - pdfs/PMC11988870.pdf\n",
      "  - pdfs/PMC3040128.pdf\n",
      "  - pdfs/PMC3177255.pdf\n",
      "  - pdfs/PMC3630201.pdf\n",
      "  - pdfs/PMC4095884.pdf\n",
      "  - pdfs/PMC4136787.pdf\n",
      "  - pdfs/PMC5387210.pdf\n",
      "  - pdfs/PMC5460236.pdf\n",
      "  - pdfs/PMC5587110.pdf\n",
      "  - pdfs/PMC5666799.pdf\n",
      "  - pdfs/PMC6222041.pdf\n",
      "  - pdfs/PMC6813909.pdf\n",
      "  - pdfs/PMC7998608.pdf\n",
      "  - pdfs/PMC8396460.pdf\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Get all PDF files in the pdfs folder\n",
    "pdfs = [str(p) for p in Path(\"pdfs\").glob(\"*.pdf\")]\n",
    "\n",
    "# Sort them alphabetically (optional)\n",
    "pdfs.sort()\n",
    "\n",
    "print(f\"Found {len(pdfs)} PDF files:\")\n",
    "for path in pdfs:\n",
    "    print(f\"  - {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef74325",
   "metadata": {},
   "source": [
    "## Step 4: Document Parsing with Unstructured\n",
    "\n",
    "Unstructured is a powerful library that can:\n",
    "- Parse PDFs with high accuracy\n",
    "- Extract tables and preserve structure\n",
    "- Extract images embedded in documents\n",
    "- Handle various document formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28b672fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 15 PDF files...\n",
      "\n",
      "================================================================================\n",
      "Processing: pdfs/PMC11500582.pdf\n",
      "This may take a few minutes depending on PDF size...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Extracted 10 elements from PDF\n",
      "\n",
      "âœ“ Successfully processed: PMC11500582.pdf\n",
      "  Elements extracted: 10\n",
      "\n",
      "Processing: pdfs/PMC11988870.pdf\n",
      "This may take a few minutes depending on PDF size...\n",
      "\n",
      "Warning: No languages specified, defaulting to English.\n",
      "âœ“ Extracted 16 elements from PDF\n",
      "\n",
      "âœ“ Successfully processed: PMC11988870.pdf\n",
      "  Elements extracted: 16\n",
      "\n",
      "Processing: pdfs/PMC3040128.pdf\n",
      "This may take a few minutes depending on PDF size...\n",
      "\n",
      "Warning: No languages specified, defaulting to English.\n",
      "âœ“ Extracted 12 elements from PDF\n",
      "\n",
      "âœ“ Successfully processed: PMC3040128.pdf\n",
      "  Elements extracted: 12\n",
      "\n",
      "Processing: pdfs/PMC3177255.pdf\n",
      "This may take a few minutes depending on PDF size...\n",
      "\n",
      "Warning: No languages specified, defaulting to English.\n",
      "âœ“ Extracted 16 elements from PDF\n",
      "\n",
      "âœ“ Successfully processed: PMC3177255.pdf\n",
      "  Elements extracted: 16\n",
      "\n",
      "Processing: pdfs/PMC3630201.pdf\n",
      "This may take a few minutes depending on PDF size...\n",
      "\n",
      "Warning: No languages specified, defaulting to English.\n",
      "âœ“ Extracted 19 elements from PDF\n",
      "\n",
      "âœ“ Successfully processed: PMC3630201.pdf\n",
      "  Elements extracted: 19\n",
      "\n",
      "Processing: pdfs/PMC4095884.pdf\n",
      "This may take a few minutes depending on PDF size...\n",
      "\n",
      "Warning: No languages specified, defaulting to English.\n",
      "âœ“ Extracted 15 elements from PDF\n",
      "\n",
      "âœ“ Successfully processed: PMC4095884.pdf\n",
      "  Elements extracted: 15\n",
      "\n",
      "Processing: pdfs/PMC4136787.pdf\n",
      "This may take a few minutes depending on PDF size...\n",
      "\n",
      "Warning: No languages specified, defaulting to English.\n",
      "âœ“ Extracted 19 elements from PDF\n",
      "\n",
      "âœ“ Successfully processed: PMC4136787.pdf\n",
      "  Elements extracted: 19\n",
      "\n",
      "Processing: pdfs/PMC5387210.pdf\n",
      "This may take a few minutes depending on PDF size...\n",
      "\n",
      "Warning: No languages specified, defaulting to English.\n",
      "âœ“ Extracted 14 elements from PDF\n",
      "\n",
      "âœ“ Successfully processed: PMC5387210.pdf\n",
      "  Elements extracted: 14\n",
      "\n",
      "Processing: pdfs/PMC5460236.pdf\n",
      "This may take a few minutes depending on PDF size...\n",
      "\n",
      "Warning: No languages specified, defaulting to English.\n",
      "âœ“ Extracted 12 elements from PDF\n",
      "\n",
      "âœ“ Successfully processed: PMC5460236.pdf\n",
      "  Elements extracted: 12\n",
      "\n",
      "Processing: pdfs/PMC5587110.pdf\n",
      "This may take a few minutes depending on PDF size...\n",
      "\n",
      "Warning: No languages specified, defaulting to English.\n",
      "âœ“ Extracted 17 elements from PDF\n",
      "\n",
      "âœ“ Successfully processed: PMC5587110.pdf\n",
      "  Elements extracted: 17\n",
      "\n",
      "Processing: pdfs/PMC5666799.pdf\n",
      "This may take a few minutes depending on PDF size...\n",
      "\n",
      "Warning: No languages specified, defaulting to English.\n",
      "âœ“ Extracted 14 elements from PDF\n",
      "\n",
      "âœ“ Successfully processed: PMC5666799.pdf\n",
      "  Elements extracted: 14\n",
      "\n",
      "Processing: pdfs/PMC6222041.pdf\n",
      "This may take a few minutes depending on PDF size...\n",
      "\n",
      "Warning: No languages specified, defaulting to English.\n",
      "âœ“ Extracted 12 elements from PDF\n",
      "\n",
      "âœ“ Successfully processed: PMC6222041.pdf\n",
      "  Elements extracted: 12\n",
      "\n",
      "Processing: pdfs/PMC6813909.pdf\n",
      "This may take a few minutes depending on PDF size...\n",
      "\n",
      "Warning: No languages specified, defaulting to English.\n",
      "âœ“ Extracted 14 elements from PDF\n",
      "\n",
      "âœ“ Successfully processed: PMC6813909.pdf\n",
      "  Elements extracted: 14\n",
      "\n",
      "Processing: pdfs/PMC7998608.pdf\n",
      "This may take a few minutes depending on PDF size...\n",
      "\n",
      "Warning: No languages specified, defaulting to English.\n",
      "âœ“ Extracted 10 elements from PDF\n",
      "\n",
      "âœ“ Successfully processed: PMC7998608.pdf\n",
      "  Elements extracted: 10\n",
      "\n",
      "Processing: pdfs/PMC8396460.pdf\n",
      "This may take a few minutes depending on PDF size...\n",
      "\n",
      "Warning: No languages specified, defaulting to English.\n",
      "âœ“ Extracted 13 elements from PDF\n",
      "\n",
      "âœ“ Successfully processed: PMC8396460.pdf\n",
      "  Elements extracted: 13\n",
      "\n",
      "================================================================================\n",
      "\n",
      "âœ“ Total PDFs processed: 15\n",
      "âœ“ Total elements extracted: 213\n",
      "\n",
      "Element types: {'CompositeElement'}\n"
     ]
    }
   ],
   "source": [
    "def load_and_parse_pdf(pdf_path: str):\n",
    "    \"\"\"\n",
    "    Parse PDF using Unstructured library.\n",
    "    Extracts text, tables, and images.\n",
    "    \"\"\"\n",
    "    print(f\"Processing: {pdf_path}\")\n",
    "    print(\"This may take a few minutes depending on PDF size...\\n\")\n",
    "    \n",
    "    # Partition PDF into elements\n",
    "    raw_pdf_elements = partition_pdf(\n",
    "        filename=pdf_path,\n",
    "        \n",
    "        # Extract tables\n",
    "        infer_table_structure=True,\n",
    "        \n",
    "        # Processing strategy\n",
    "        strategy=\"hi_res\",  # High resolution for better quality\n",
    "        \n",
    "        # Extract images\n",
    "        extract_images_in_pdf=True,\n",
    "        extract_image_block_types=[\"Image\", \"Table\"],\n",
    "        extract_image_block_to_payload=True,\n",
    "        \n",
    "        # Chunking strategy\n",
    "        chunking_strategy=\"by_title\",\n",
    "        max_characters=10000,\n",
    "        combine_text_under_n_chars=2000,\n",
    "        new_after_n_chars=6000,\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ“ Extracted {len(raw_pdf_elements)} elements from PDF\\n\")\n",
    "    \n",
    "    return raw_pdf_elements\n",
    "\n",
    "def process_multiple_pdfs(pdf_paths: List[str]):\n",
    "    \"\"\"\n",
    "    Process multiple PDFs and return all elements with source tracking.\n",
    "    \n",
    "    Args:\n",
    "        pdf_paths: List of PDF file paths to process\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with source filename as key and elements as value\n",
    "    \"\"\"\n",
    "    all_pdf_elements = {}\n",
    "    \n",
    "    print(f\"Processing {len(pdf_paths)} PDF files...\\n\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for pdf_path in pdf_paths:\n",
    "        if Path(pdf_path).exists():\n",
    "            try:\n",
    "                elements = load_and_parse_pdf(pdf_path)\n",
    "                filename = Path(pdf_path).name\n",
    "                all_pdf_elements[filename] = elements\n",
    "                print(f\"âœ“ Successfully processed: {filename}\")\n",
    "                print(f\"  Elements extracted: {len(elements)}\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"âœ— Error processing {pdf_path}: {e}\\n\")\n",
    "        else:\n",
    "            print(f\"âœ— File not found: {pdf_path}\\n\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nâœ“ Total PDFs processed: {len(all_pdf_elements)}\")\n",
    "    total_elements = sum(len(elements) for elements in all_pdf_elements.values())\n",
    "    print(f\"âœ“ Total elements extracted: {total_elements}\\n\")\n",
    "    \n",
    "    return all_pdf_elements\n",
    "\n",
    "# Example: Single PDF processing (original behavior)\n",
    "pdf_path = \"paper.pdf\"  # Change this to your PDF file\n",
    "\n",
    "# Example: Multiple PDFs processing (new feature)\n",
    "pdf_paths = pdfs\n",
    "\n",
    "# Choose single or batch processing\n",
    "use_batch_mode = True  # Set to False for single PDF\n",
    "\n",
    "if use_batch_mode:\n",
    "    # Process multiple PDFs\n",
    "    all_elements_by_source = process_multiple_pdfs(pdf_paths)\n",
    "    \n",
    "    # Flatten all elements for backward compatibility\n",
    "    elements = []\n",
    "    for source, source_elements in all_elements_by_source.items():\n",
    "        # Add source metadata to each element\n",
    "        for elem in source_elements:\n",
    "            if hasattr(elem, 'metadata'):\n",
    "                elem.metadata.source_file = source\n",
    "            elements.append(elem)\n",
    "    \n",
    "    if elements:\n",
    "        print(f\"Element types: {set([str(type(el).__name__) for el in elements])}\")\n",
    "else:\n",
    "    # Process single PDF (original behavior)\n",
    "    if Path(pdf_path).exists():\n",
    "        elements = load_and_parse_pdf(pdf_path)\n",
    "        # Add source metadata\n",
    "        for elem in elements:\n",
    "            if hasattr(elem, 'metadata'):\n",
    "                elem.metadata.source_file = Path(pdf_path).name\n",
    "        all_elements_by_source = {Path(pdf_path).name: elements}\n",
    "        print(f\"Element types: {set([str(type(el).__name__) for el in elements])}\")\n",
    "    else:\n",
    "        print(f\"âœ— File not found: {pdf_path}\")\n",
    "        print(\"Please update the pdf_path variable with your PDF file\")\n",
    "        elements = []\n",
    "        all_elements_by_source = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be82a2a4",
   "metadata": {},
   "source": [
    "## Step 5: Categorize Elements by Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6b3bbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Categorized elements (before filtering):\n",
      "  Text chunks: 213\n",
      "  Tables: 28\n",
      "  Images: 184\n",
      "\n",
      "âœ“ After filtering out 'unknown' sources:\n",
      "  Text chunks: 213\n",
      "  Tables: 28\n",
      "  Images: 184\n",
      "\n",
      "ğŸ“ Breakdown by source file:\n",
      "  PMC11500582.pdf:\n",
      "    - Text chunks: 10\n",
      "    - Tables: 0\n",
      "    - Images: 6\n",
      "  PMC11988870.pdf:\n",
      "    - Text chunks: 16\n",
      "    - Tables: 1\n",
      "    - Images: 6\n",
      "  PMC3040128.pdf:\n",
      "    - Text chunks: 12\n",
      "    - Tables: 2\n",
      "    - Images: 44\n",
      "  PMC3177255.pdf:\n",
      "    - Text chunks: 16\n",
      "    - Tables: 1\n",
      "    - Images: 8\n",
      "  PMC3630201.pdf:\n",
      "    - Text chunks: 19\n",
      "    - Tables: 4\n",
      "    - Images: 7\n",
      "  PMC4095884.pdf:\n",
      "    - Text chunks: 15\n",
      "    - Tables: 0\n",
      "    - Images: 9\n",
      "  PMC4136787.pdf:\n",
      "    - Text chunks: 19\n",
      "    - Tables: 6\n",
      "    - Images: 12\n",
      "  PMC5387210.pdf:\n",
      "    - Text chunks: 14\n",
      "    - Tables: 0\n",
      "    - Images: 9\n",
      "  PMC5460236.pdf:\n",
      "    - Text chunks: 12\n",
      "    - Tables: 2\n",
      "    - Images: 4\n",
      "  PMC5587110.pdf:\n",
      "    - Text chunks: 17\n",
      "    - Tables: 3\n",
      "    - Images: 25\n",
      "  PMC5666799.pdf:\n",
      "    - Text chunks: 14\n",
      "    - Tables: 3\n",
      "    - Images: 23\n",
      "  PMC6222041.pdf:\n",
      "    - Text chunks: 12\n",
      "    - Tables: 2\n",
      "    - Images: 9\n",
      "  PMC6813909.pdf:\n",
      "    - Text chunks: 14\n",
      "    - Tables: 0\n",
      "    - Images: 5\n",
      "  PMC7998608.pdf:\n",
      "    - Text chunks: 10\n",
      "    - Tables: 2\n",
      "    - Images: 11\n",
      "  PMC8396460.pdf:\n",
      "    - Text chunks: 13\n",
      "    - Tables: 2\n",
      "    - Images: 6\n"
     ]
    }
   ],
   "source": [
    "def categorize_elements(raw_elements):\n",
    "    \"\"\"\n",
    "    Categorize elements into text, tables, and images.\n",
    "    Preserves source file metadata.\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    \n",
    "    for element in raw_elements:\n",
    "        element_type = str(type(element).__name__)\n",
    "        \n",
    "        if element_type == \"CompositeElement\":\n",
    "            # Text element - preserve as object to keep metadata\n",
    "            texts.append(element)\n",
    "    \n",
    "    return texts\n",
    "\n",
    "# Get the tables from the CompositeElement objects\n",
    "def get_tables(chunks):\n",
    "    tables = []\n",
    "    for chunk in chunks:\n",
    "        if \"CompositeElement\" in str(type(chunk)):\n",
    "            chunk_els = chunk.metadata.orig_elements\n",
    "            for el in chunk_els:\n",
    "                if \"Table\" in str(type(el)):\n",
    "                    # Store table with source metadata\n",
    "                    source_file = getattr(chunk.metadata, 'source_file', 'unknown')\n",
    "                    tables.append({\n",
    "                        'content': el.metadata.text_as_html,\n",
    "                        'source': source_file\n",
    "                    })\n",
    "    return tables\n",
    "\n",
    "# Get the images from the CompositeElement objects\n",
    "def get_images_base64(chunks):\n",
    "    images_b64 = []\n",
    "    for chunk in chunks:\n",
    "        if \"CompositeElement\" in str(type(chunk)):\n",
    "            chunk_els = chunk.metadata.orig_elements\n",
    "            for el in chunk_els:\n",
    "                if \"Image\" in str(type(el)):\n",
    "                    # Store image with source metadata\n",
    "                    source_file = getattr(chunk.metadata, 'source_file', 'unknown')\n",
    "                    images_b64.append({\n",
    "                        'content': el.metadata.image_base64,\n",
    "                        'source': source_file\n",
    "                    })\n",
    "    return images_b64\n",
    "\n",
    "def filter_unknown_sources(texts, tables, images):\n",
    "    \"\"\"\n",
    "    Filter out all elements with 'unknown' source.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text elements (CompositeElement objects with metadata)\n",
    "        tables: List of table dicts with 'content' and 'source' keys\n",
    "        images: List of image dicts with 'content' and 'source' keys\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of filtered (texts, tables, images)\n",
    "    \"\"\"\n",
    "    # Filter texts - check metadata.source_file\n",
    "    filtered_texts = [\n",
    "        text for text in texts \n",
    "        if hasattr(text, 'metadata') and \n",
    "        getattr(text.metadata, 'source_file', 'unknown') != 'unknown'\n",
    "    ]\n",
    "    \n",
    "    # Filter tables - check 'source' key\n",
    "    filtered_tables = [\n",
    "        table for table in tables \n",
    "        if isinstance(table, dict) and table.get('source', 'unknown') != 'unknown'\n",
    "    ]\n",
    "    \n",
    "    # Filter images - check 'source' key\n",
    "    filtered_images = [\n",
    "        image for image in images \n",
    "        if isinstance(image, dict) and image.get('source', 'unknown') != 'unknown'\n",
    "    ]\n",
    "    \n",
    "    return filtered_texts, filtered_tables, filtered_images\n",
    "\n",
    "if elements:\n",
    "    texts = categorize_elements(elements)\n",
    "    tables = get_tables(elements)\n",
    "    images = get_images_base64(elements)\n",
    "    \n",
    "    print(f\"âœ“ Categorized elements (before filtering):\")\n",
    "    print(f\"  Text chunks: {len(texts)}\")\n",
    "    print(f\"  Tables: {len(tables)}\")\n",
    "    print(f\"  Images: {len(images)}\")\n",
    "    \n",
    "    # Filter out unknown sources\n",
    "    texts, tables, images = filter_unknown_sources(texts, tables, images)\n",
    "    \n",
    "    print(f\"\\nâœ“ After filtering out 'unknown' sources:\")\n",
    "    print(f\"  Text chunks: {len(texts)}\")\n",
    "    print(f\"  Tables: {len(tables)}\")\n",
    "    print(f\"  Images: {len(images)}\")\n",
    "    \n",
    "    # Show breakdown by source file\n",
    "    if all_elements_by_source:\n",
    "        print(f\"\\nğŸ“ Breakdown by source file:\")\n",
    "        for source, source_elements in all_elements_by_source.items():\n",
    "            source_texts = categorize_elements(source_elements)\n",
    "            source_tables = get_tables(source_elements)\n",
    "            source_images = get_images_base64(source_elements)\n",
    "            # Filter each source's elements\n",
    "            source_texts, source_tables, source_images = filter_unknown_sources(\n",
    "                source_texts, source_tables, source_images\n",
    "            )\n",
    "            print(f\"  {source}:\")\n",
    "            print(f\"    - Text chunks: {len(source_texts)}\")\n",
    "            print(f\"    - Tables: {len(source_tables)}\")\n",
    "            print(f\"    - Images: {len(source_images)}\")\n",
    "else:\n",
    "    texts, tables, images = [], [], []\n",
    "    print(\"No elements to categorize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac80e633",
   "metadata": {},
   "source": [
    "## Step 6: Generate Summaries for All Elements\n",
    "\n",
    "We'll generate summaries for:\n",
    "- Text chunks\n",
    "- Tables (describe structure and content)\n",
    "- Images (use vision model if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72a836a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Text and table summarization chain ready\n",
      "âœ“ Image description chain ready\n"
     ]
    }
   ],
   "source": [
    "# Prompt for text and table summarization\n",
    "prompt_text = \"\"\"\n",
    "You are an assistant tasked with summarizing tables and text.\n",
    "Give a concise comprehensive summary of the table or text.\n",
    "\n",
    "Respond only with the summary, no additional comment.\n",
    "Do not start your message by saying \"Here is a summary\" or anything like that.\n",
    "Just give the summary as it is.\n",
    "\n",
    "HTML Table or text chunk: {element}\n",
    "\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "# Summary chain for text and tables\n",
    "model = ChatOllama(temperature=0.2, model=\"gemma3:4b\")\n",
    "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
    "\n",
    "print(\"âœ“ Text and table summarization chain ready\")\n",
    "\n",
    "# Image description prompt\n",
    "prompt_template = \"\"\"Describe the image in detail. For context,\n",
    "                  the image is part of a research paper. Be specific about graphs, such as bar plots.\"\"\"\n",
    "messages = [\n",
    "    (\n",
    "        \"user\",\n",
    "        [\n",
    "            {\"type\": \"text\", \"text\": prompt_template},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": \"data:image/jpeg;base64,{image}\"},\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "]\n",
    "\n",
    "image_prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "# Image description chain\n",
    "image_chain = image_prompt | ChatOllama(model=\"llava:7b\") | StrOutputParser()\n",
    "\n",
    "print(\"âœ“ Image description chain ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0b78dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summaries...\n",
      "\n",
      "Summarizing 213 text chunks...\n",
      "âœ“ Generated 213 text summaries\n",
      "Summarizing 28 tables...\n",
      "âœ“ Generated 28 table summaries\n",
      "Describing 184 images...\n",
      "âœ“ Generated 184 image summaries\n",
      "\n",
      "âœ“ Summarization complete!\n",
      "   Total summaries: 425\n"
     ]
    }
   ],
   "source": [
    "# Generate summaries for all elements using batch processing\n",
    "print(\"Generating summaries...\\n\")\n",
    "\n",
    "text_summaries = []\n",
    "table_summaries = []\n",
    "image_summaries = []\n",
    "\n",
    "# Summarize texts using batch processing\n",
    "if texts:\n",
    "    print(f\"Summarizing {len(texts)} text chunks...\")\n",
    "    # Extract text content from CompositeElement objects\n",
    "    text_contents = [text_elem.text if hasattr(text_elem, 'text') else str(text_elem) for text_elem in texts]\n",
    "    text_summaries = summarize_chain.batch(text_contents, {\"max_concurrency\": 3})\n",
    "    print(f\"âœ“ Generated {len(text_summaries)} text summaries\")\n",
    "\n",
    "# Summarize tables using batch processing\n",
    "if tables:\n",
    "    print(f\"Summarizing {len(tables)} tables...\")\n",
    "    # Extract table content\n",
    "    table_contents = [table_dict['content'] if isinstance(table_dict, dict) else table_dict for table_dict in tables]\n",
    "    table_summaries = summarize_chain.batch(table_contents, {\"max_concurrency\": 3})\n",
    "    print(f\"âœ“ Generated {len(table_summaries)} table summaries\")\n",
    "\n",
    "# Summarize images using batch processing\n",
    "if images:\n",
    "    print(f\"Describing {len(images)} images...\")\n",
    "    try:\n",
    "        # Extract image content\n",
    "        image_contents = [image_dict['content'] if isinstance(image_dict, dict) else image_dict for image_dict in images]\n",
    "        image_summaries = image_chain.batch(image_contents)\n",
    "        print(f\"âœ“ Generated {len(image_summaries)} image summaries\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš  Error with vision model: {e}\")\n",
    "        print(\"Falling back to placeholder descriptions...\")\n",
    "        image_summaries = [f\"Image content (vision model not available)\" for _ in images]\n",
    "\n",
    "print(f\"\\nâœ“ Summarization complete!\")\n",
    "print(f\"   Total summaries: {len(text_summaries) + len(table_summaries) + len(image_summaries)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ac0e05",
   "metadata": {},
   "source": [
    "## Step 7: Add to MultiVectorRetriever\n",
    "\n",
    "Store:\n",
    "- **Summaries** â†’ Vector store (for retrieval)\n",
    "- **Raw content** â†’ Document store (for LLM context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae2b0184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Added 213 text documents\n",
      "âœ“ Added 28 table documents\n",
      "âœ“ Added 184 image documents\n",
      "\n",
      "âœ“ All documents added to retriever!\n",
      "\n",
      "ğŸ“Š Documents by source file:\n",
      "  PMC11500582.pdf: 16 documents\n",
      "    - Texts: 10, Tables: 0, Images: 6\n",
      "  PMC11988870.pdf: 23 documents\n",
      "    - Texts: 16, Tables: 1, Images: 6\n",
      "  PMC3040128.pdf: 58 documents\n",
      "    - Texts: 12, Tables: 2, Images: 44\n",
      "  PMC3177255.pdf: 25 documents\n",
      "    - Texts: 16, Tables: 1, Images: 8\n",
      "  PMC3630201.pdf: 30 documents\n",
      "    - Texts: 19, Tables: 4, Images: 7\n",
      "  PMC4095884.pdf: 24 documents\n",
      "    - Texts: 15, Tables: 0, Images: 9\n",
      "  PMC4136787.pdf: 37 documents\n",
      "    - Texts: 19, Tables: 6, Images: 12\n",
      "  PMC5387210.pdf: 23 documents\n",
      "    - Texts: 14, Tables: 0, Images: 9\n",
      "  PMC5460236.pdf: 18 documents\n",
      "    - Texts: 12, Tables: 2, Images: 4\n",
      "  PMC5587110.pdf: 45 documents\n",
      "    - Texts: 17, Tables: 3, Images: 25\n",
      "  PMC5666799.pdf: 40 documents\n",
      "    - Texts: 14, Tables: 3, Images: 23\n",
      "  PMC6222041.pdf: 23 documents\n",
      "    - Texts: 12, Tables: 2, Images: 9\n",
      "  PMC6813909.pdf: 19 documents\n",
      "    - Texts: 14, Tables: 0, Images: 5\n",
      "  PMC7998608.pdf: 23 documents\n",
      "    - Texts: 10, Tables: 2, Images: 11\n",
      "  PMC8396460.pdf: 21 documents\n",
      "    - Texts: 13, Tables: 2, Images: 6\n"
     ]
    }
   ],
   "source": [
    "def add_documents_to_retriever(texts, text_summaries, tables, table_summaries, images, image_summaries):\n",
    "    \"\"\"\n",
    "    Add all documents to the MultiVectorRetriever.\n",
    "    Now includes source file metadata for multi-PDF tracking.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add text documents\n",
    "    text_ids = [str(uuid.uuid4()) for _ in texts]\n",
    "    \n",
    "    if texts:\n",
    "        # Store summaries in vector store with source metadata\n",
    "        summary_docs = []\n",
    "        for i, (text_elem, summary) in enumerate(zip(texts, text_summaries)):\n",
    "            source_file = getattr(text_elem.metadata, 'source_file', 'unknown') if hasattr(text_elem, 'metadata') else 'unknown'\n",
    "            summary_docs.append(\n",
    "                Document(\n",
    "                    page_content=summary, \n",
    "                    metadata={\n",
    "                        id_key: text_ids[i], \n",
    "                        \"type\": \"text\",\n",
    "                        \"source_file\": source_file\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "        retriever.vectorstore.add_documents(summary_docs)\n",
    "        \n",
    "        # Store raw content in docstore\n",
    "        text_contents = [elem.text if hasattr(elem, 'text') else str(elem) for elem in texts]\n",
    "        retriever.docstore.mset(list(zip(text_ids, text_contents)))\n",
    "        print(f\"âœ“ Added {len(texts)} text documents\")\n",
    "    \n",
    "    # Add table documents\n",
    "    table_ids = [str(uuid.uuid4()) for _ in tables]\n",
    "    \n",
    "    if tables:\n",
    "        # Store summaries in vector store with source metadata\n",
    "        summary_docs = []\n",
    "        for i, (table_dict, summary) in enumerate(zip(tables, table_summaries)):\n",
    "            source_file = table_dict.get('source', 'unknown') if isinstance(table_dict, dict) else 'unknown'\n",
    "            summary_docs.append(\n",
    "                Document(\n",
    "                    page_content=summary,\n",
    "                    metadata={\n",
    "                        id_key: table_ids[i],\n",
    "                        \"type\": \"table\",\n",
    "                        \"source_file\": source_file\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "        retriever.vectorstore.add_documents(summary_docs)\n",
    "        \n",
    "        # Store raw content in docstore\n",
    "        table_contents = [t['content'] if isinstance(t, dict) else t for t in tables]\n",
    "        retriever.docstore.mset(list(zip(table_ids, table_contents)))\n",
    "        print(f\"âœ“ Added {len(tables)} table documents\")\n",
    "    \n",
    "    # Add image documents\n",
    "    image_ids = [str(uuid.uuid4()) for _ in images]\n",
    "    \n",
    "    if images:\n",
    "        # Store summaries in vector store with source metadata\n",
    "        summary_docs = []\n",
    "        for i, (image_dict, summary) in enumerate(zip(images, image_summaries)):\n",
    "            source_file = image_dict.get('source', 'unknown') if isinstance(image_dict, dict) else 'unknown'\n",
    "            summary_docs.append(\n",
    "                Document(\n",
    "                    page_content=summary,\n",
    "                    metadata={\n",
    "                        id_key: image_ids[i],\n",
    "                        \"type\": \"image\",\n",
    "                        \"source_file\": source_file\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "        retriever.vectorstore.add_documents(summary_docs)\n",
    "        \n",
    "        # Store raw content (base64) in docstore\n",
    "        image_contents = [img['content'] if isinstance(img, dict) else img for img in images]\n",
    "        retriever.docstore.mset(list(zip(image_ids, image_contents)))\n",
    "        print(f\"âœ“ Added {len(images)} image documents\")\n",
    "    \n",
    "    # Show summary by source file\n",
    "    if texts or tables or images:\n",
    "        print(\"\\nâœ“ All documents added to retriever!\")\n",
    "        print(\"\\nğŸ“Š Documents by source file:\")\n",
    "        \n",
    "        # Collect source statistics\n",
    "        source_stats = {}\n",
    "        \n",
    "        # Count texts by source\n",
    "        for text_elem in texts:\n",
    "            source = getattr(text_elem.metadata, 'source_file', 'unknown') if hasattr(text_elem, 'metadata') else 'unknown'\n",
    "            source_stats[source] = source_stats.get(source, {'texts': 0, 'tables': 0, 'images': 0})\n",
    "            source_stats[source]['texts'] += 1\n",
    "        \n",
    "        # Count tables by source\n",
    "        for table_dict in tables:\n",
    "            source = table_dict.get('source', 'unknown') if isinstance(table_dict, dict) else 'unknown'\n",
    "            source_stats[source] = source_stats.get(source, {'texts': 0, 'tables': 0, 'images': 0})\n",
    "            source_stats[source]['tables'] += 1\n",
    "        \n",
    "        # Count images by source\n",
    "        for image_dict in images:\n",
    "            source = image_dict.get('source', 'unknown') if isinstance(image_dict, dict) else 'unknown'\n",
    "            source_stats[source] = source_stats.get(source, {'texts': 0, 'tables': 0, 'images': 0})\n",
    "            source_stats[source]['images'] += 1\n",
    "        \n",
    "        # Display statistics\n",
    "        for source, stats in source_stats.items():\n",
    "            total = stats['texts'] + stats['tables'] + stats['images']\n",
    "            print(f\"  {source}: {total} documents\")\n",
    "            print(f\"    - Texts: {stats['texts']}, Tables: {stats['tables']}, Images: {stats['images']}\")\n",
    "\n",
    "# Add all documents\n",
    "if texts or tables or images:\n",
    "    add_documents_to_retriever(texts, text_summaries, tables, table_summaries, images, image_summaries)\n",
    "else:\n",
    "    print(\"No documents to add\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7743aa96",
   "metadata": {},
   "source": [
    "## Step 8: Create RAG Chain\n",
    "\n",
    "Build a LangChain RAG pipeline that:\n",
    "1. Retrieves relevant documents\n",
    "2. Formats context\n",
    "3. Generates grounded answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c236d9",
   "metadata": {},
   "source": [
    "## Step 7.5: Query Specific Documents\n",
    "\n",
    "When working with multiple PDFs, you can filter results by source file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a724c384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ RAG chain created!\n",
      "âœ“ Ready to answer questions from multiple PDFs\n"
     ]
    }
   ],
   "source": [
    "# Create prompt template\n",
    "template = \"\"\"Answer the question based on the following context. Don't cite any sources in your response. Don't say \"Based on the resource\" or any other simialar sentences. Respond with Markdown format.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Create RAG chain\n",
    "def format_docs(docs):\n",
    "    \"\"\"Format retrieved documents for the prompt.\"\"\"\n",
    "    formatted = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        content = doc if isinstance(doc, str) else str(doc)\n",
    "        formatted.append(f\"[Source {i+1}]\\n{content[:1000]}...\\n\")\n",
    "    return \"\\n\\n\".join(formatted)\n",
    "\n",
    "def format_docs_with_source(docs, metadata_list):\n",
    "    \"\"\"Format retrieved documents with source file information.\"\"\"\n",
    "    formatted = []\n",
    "    for i, (doc, meta) in enumerate(zip(docs, metadata_list)):\n",
    "        content = doc if isinstance(doc, str) else str(doc)\n",
    "        source = meta.get('source_file', 'unknown') if meta else 'unknown'\n",
    "        formatted.append(f\"[Source {i+1} - from {source}]\\n{content[:1000]}...\\n\")\n",
    "    return \"\\n\\n\".join(formatted)\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"âœ“ RAG chain created!\")\n",
    "print(\"âœ“ Ready to answer questions from multiple PDFs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a20e15",
   "metadata": {},
   "source": [
    "## Step 9: Query the System\n",
    "\n",
    "Now you can ask questions about your documents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11d52f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try asking questions about your documents!\n",
      "\n",
      "Examples:\n",
      "  query(\"What is this document about?\")\n",
      "  query(\"What are the main findings?\")\n",
      "  query(\"Are there any tables or data?\")\n",
      "  query(\"Compare information across documents\")\n",
      "\n",
      "For source-specific queries:\n",
      "  query_by_source(\"What does this paper discuss?\", source_file=\"paper.pdf\")\n"
     ]
    }
   ],
   "source": [
    "def query(question: str, show_sources: bool = True):\n",
    "    \"\"\"Query the RAG system and optionally show source files.\"\"\"\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Get answer\n",
    "    answer = rag_chain.invoke(question)\n",
    "    \n",
    "    print(f\"\\nAnswer:\\n{answer}\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # Show retrieved documents with sources\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    print(f\"\\nğŸ“š Retrieved {len(docs)} relevant documents\")\n",
    "    \n",
    "    if show_sources:\n",
    "        # Get metadata from vector store\n",
    "        results = retriever.vectorstore.similarity_search(question, k=5)\n",
    "        sources = {}\n",
    "        for r in results:\n",
    "            source = r.metadata.get('source_file', 'unknown')\n",
    "            doc_type = r.metadata.get('type', 'unknown')\n",
    "            if source not in sources:\n",
    "                sources[source] = []\n",
    "            sources[source].append(doc_type)\n",
    "        \n",
    "        print(f\"\\nğŸ“ Sources used:\")\n",
    "        for source, types in sources.items():\n",
    "            type_counts = {}\n",
    "            for t in types:\n",
    "                type_counts[t] = type_counts.get(t, 0) + 1\n",
    "            type_str = \", \".join([f\"{count} {type}\" for type, count in type_counts.items()])\n",
    "            print(f\"  â€¢ {source}: {type_str}\")\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Example queries\n",
    "if texts or tables or images:\n",
    "    print(\"Try asking questions about your documents!\\n\")\n",
    "    print(\"Examples:\")\n",
    "    print('  query(\"What is this document about?\")')\n",
    "    print('  query(\"What are the main findings?\")')\n",
    "    print('  query(\"Are there any tables or data?\")')\n",
    "    print('  query(\"Compare information across documents\")')\n",
    "    print('\\nFor source-specific queries:')\n",
    "    print('  query_by_source(\"What does this paper discuss?\", source_file=\"paper.pdf\")')\n",
    "else:\n",
    "    print(\"Add documents first before querying\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05909db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: what do you know about Mice experiments?\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      "The provided sources detail experiments involving RNA sequencing with various mouse groups. Specifically, there are data from â€œFlight experiment (SF),â€ â€œFlight vivarium control (SFV),â€ and â€œGround control experiment (GC)â€ groups. The â€œGround control experiment - vivarium control (GCV)â€ group is also mentioned. The RNA sequencing utilized different fluorescent labels (FAM, HEX, Cy5) across various experimental conditions, including singleplex, duplex, and triplex configurations. Data includes measurements like â€œE. coliRNA,â€ â€œdnaK-FAM,â€ â€œrpoA-HEX,â€ and â€œsrIR-Cy5,â€ alongside numerical values representing expression levels.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ğŸ“š Retrieved 3 relevant documents\n",
      "\n",
      "ğŸ“ Sources used:\n",
      "  â€¢ PMC4136787.pdf: 1 table\n",
      "  â€¢ paper2.pdf: 1 table\n",
      "  â€¢ PMC8396460.pdf: 1 text\n",
      "  â€¢ PMC5587110.pdf: 1 table\n",
      "  â€¢ PMC7998608.pdf: 1 table\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The provided sources detail experiments involving RNA sequencing with various mouse groups. Specifically, there are data from â€œFlight experiment (SF),â€ â€œFlight vivarium control (SFV),â€ and â€œGround control experiment (GC)â€ groups. The â€œGround control experiment - vivarium control (GCV)â€ group is also mentioned. The RNA sequencing utilized different fluorescent labels (FAM, HEX, Cy5) across various experimental conditions, including singleplex, duplex, and triplex configurations. Data includes measurements like â€œE. coliRNA,â€ â€œdnaK-FAM,â€ â€œrpoA-HEX,â€ and â€œsrIR-Cy5,â€ alongside numerical values representing expression levels.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example query 1\n",
    "query(\"what do you know about Mice experiments?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da11ee30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What are the key findings or results or coclusions?\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      "The provided sources do not contain any key findings, results, or conclusions. They consist of gene primer sequences, a gene list enrichment analysis reference, and a table of gene fold regulations, alongside a seemingly nonsensical table with no apparent data.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ğŸ“š Retrieved 4 relevant documents\n",
      "\n",
      "ğŸ“ Sources used:\n",
      "  â€¢ PMC7998608.pdf: 1 table\n",
      "  â€¢ PMC8396460.pdf: 1 text, 1 table\n",
      "  â€¢ PMC5460236.pdf: 1 table, 1 text\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The provided sources do not contain any key findings, results, or conclusions. They consist of gene primer sequences, a gene list enrichment analysis reference, and a table of gene fold regulations, alongside a seemingly nonsensical table with no apparent data.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example query 2\n",
    "query(\"What are the key findings or results or coclusions?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ca9c2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What are the experiments performed on mice?\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      "The table in Source 3 details experiments performed on mice, specifically within the â€œFlight experiment (SF)â€ and â€œFlight vivarium control (SFV)â€ groups. These experiments involved RNA isolation and multiplex quantitative real-time PCR analysis of gene expression.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ğŸ“š Retrieved 3 relevant documents\n",
      "\n",
      "ğŸ“ Sources used:\n",
      "  â€¢ PMC4136787.pdf: 1 table, 1 text\n",
      "  â€¢ paper2.pdf: 1 table\n",
      "  â€¢ PMC5587110.pdf: 1 table, 1 text\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The table in Source 3 details experiments performed on mice, specifically within the â€œFlight experiment (SF)â€ and â€œFlight vivarium control (SFV)â€ groups. These experiments involved RNA isolation and multiplex quantitative real-time PCR analysis of gene expression.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example query 3 - Custom question\n",
    "query(\"What are the experiments performed on mice?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "356b7442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export Functions Available:\n",
      "\n",
      "1. Export vector embeddings only:\n",
      "   export_chroma_to_csv(\"my_export.csv\")\n",
      "\n",
      "2. Export raw content only:\n",
      "   export_raw_content_to_csv(\"raw_content.csv\")\n",
      "\n",
      "3. Export complete dataset (recommended):\n",
      "   export_complete_dataset(\"nasa_rag_export\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "def export_chroma_to_csv(output_file: str = \"chroma_export.csv\"):\n",
    "    \"\"\"\n",
    "    Export ChromaDB contents to CSV format for PostgreSQL/pgvector import.\n",
    "    \n",
    "    Exports:\n",
    "    - Document ID\n",
    "    - Content (summary)\n",
    "    - Embedding vector\n",
    "    - Metadata (type, source_file)\n",
    "    - Original content ID (for linking to raw content)\n",
    "    \n",
    "    Args:\n",
    "        output_file: Path to output CSV file\n",
    "    \"\"\"\n",
    "    print(f\"Exporting ChromaDB to {output_file}...\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    try:\n",
    "        # Get all documents from vectorstore with embeddings\n",
    "        collection = retriever.vectorstore._collection\n",
    "        \n",
    "        # Get all data from the collection\n",
    "        results = collection.get(\n",
    "            include=['embeddings', 'documents', 'metadatas']\n",
    "        )\n",
    "        \n",
    "        # Prepare data for CSV\n",
    "        rows = []\n",
    "        for i in range(len(results['ids'])):\n",
    "            # Convert numpy array to list for JSON serialization\n",
    "            embedding = results['embeddings'][i]\n",
    "            if isinstance(embedding, np.ndarray):\n",
    "                embedding = embedding.tolist()\n",
    "            elif not isinstance(embedding, list):\n",
    "                embedding = list(embedding)\n",
    "            \n",
    "            row = {\n",
    "                'id': results['ids'][i],\n",
    "                'content': results['documents'][i],\n",
    "                'embedding': json.dumps(embedding),  # Now serializable\n",
    "                'metadata': json.dumps(results['metadatas'][i]),  # Store metadata as JSON\n",
    "                'doc_type': results['metadatas'][i].get('type', 'unknown'),\n",
    "                'source_file': results['metadatas'][i].get('source_file', 'unknown'),\n",
    "                'doc_id': results['metadatas'][i].get(id_key, ''),  # Link to raw content\n",
    "                'created_at': datetime.now().isoformat()\n",
    "            }\n",
    "            rows.append(row)\n",
    "        \n",
    "        # Write to CSV\n",
    "        if rows:\n",
    "            fieldnames = ['id', 'content', 'embedding', 'metadata', 'doc_type', \n",
    "                         'source_file', 'doc_id', 'created_at']\n",
    "            \n",
    "            with open(output_file, 'w', newline='', encoding='utf-8') as f:\n",
    "                writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_MINIMAL)\n",
    "                writer.writeheader()\n",
    "                writer.writerows(rows)\n",
    "            \n",
    "            print(f\"âœ“ Exported {len(rows)} documents to {output_file}\")\n",
    "            \n",
    "            # Get embedding dimension\n",
    "            if rows:\n",
    "                first_embedding = json.loads(rows[0]['embedding'])\n",
    "                print(f\"âœ“ Embedding dimension: {len(first_embedding)}\")\n",
    "            \n",
    "            print(f\"\\nFile structure:\")\n",
    "            print(f\"  - id: Unique document identifier\")\n",
    "            print(f\"  - content: Document summary text\")\n",
    "            print(f\"  - embedding: Vector embedding (JSON array of floats)\")\n",
    "            print(f\"  - metadata: Full metadata (JSON)\")\n",
    "            print(f\"  - doc_type: Type (text/table/image)\")\n",
    "            print(f\"  - source_file: Original PDF filename\")\n",
    "            print(f\"  - doc_id: Link to raw content\")\n",
    "            print(f\"  - created_at: Export timestamp\")\n",
    "            \n",
    "            # Show statistics\n",
    "            by_type = {}\n",
    "            by_source = {}\n",
    "            for row in rows:\n",
    "                doc_type = row['doc_type']\n",
    "                source = row['source_file']\n",
    "                by_type[doc_type] = by_type.get(doc_type, 0) + 1\n",
    "                by_source[source] = by_source.get(source, 0) + 1\n",
    "            \n",
    "            print(f\"\\nğŸ“Š Export Statistics:\")\n",
    "            print(f\"  Total documents: {len(rows)}\")\n",
    "            print(f\"  By type: {by_type}\")\n",
    "            print(f\"  By source: {by_source}\")\n",
    "            \n",
    "        else:\n",
    "            print(\"âš  No documents found in ChromaDB\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error exporting: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "def export_raw_content_to_csv(output_file: str = \"raw_content_export.csv\"):\n",
    "    \"\"\"\n",
    "    Export raw document content from docstore to CSV.\n",
    "    This includes the full text/tables/images that aren't in the vector DB.\n",
    "    \n",
    "    Args:\n",
    "        output_file: Path to output CSV file\n",
    "    \"\"\"\n",
    "    print(f\"Exporting raw content to {output_file}...\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    try:\n",
    "        # Get all doc_ids from vectorstore\n",
    "        collection = retriever.vectorstore._collection\n",
    "        results = collection.get(include=['metadatas'])\n",
    "        \n",
    "        doc_ids = [meta.get(id_key) for meta in results['metadatas'] if meta.get(id_key)]\n",
    "        \n",
    "        # Get raw content from docstore\n",
    "        raw_contents = retriever.docstore.mget(doc_ids)\n",
    "        \n",
    "        # Prepare data for CSV\n",
    "        rows = []\n",
    "        for doc_id, content in zip(doc_ids, raw_contents):\n",
    "            if content:\n",
    "                # Find corresponding metadata\n",
    "                meta = next((m for m in results['metadatas'] if m.get(id_key) == doc_id), {})\n",
    "                \n",
    "                # Handle different content types\n",
    "                content_str = content\n",
    "                if isinstance(content, bytes):\n",
    "                    # For binary content (images), encode as base64\n",
    "                    import base64\n",
    "                    content_str = base64.b64encode(content).decode('utf-8')\n",
    "                elif not isinstance(content, str):\n",
    "                    content_str = str(content)\n",
    "                \n",
    "                row = {\n",
    "                    'doc_id': doc_id,\n",
    "                    'content': content_str,\n",
    "                    'doc_type': meta.get('type', 'unknown'),\n",
    "                    'source_file': meta.get('source_file', 'unknown'),\n",
    "                    'created_at': datetime.now().isoformat()\n",
    "                }\n",
    "                if row['source_file'] != 'unknown':\n",
    "                    rows.append(row)\n",
    "        \n",
    "        # Write to CSV\n",
    "        if rows:\n",
    "            fieldnames = ['doc_id', 'content', 'doc_type', 'source_file', 'created_at']\n",
    "            \n",
    "            with open(output_file, 'w', newline='', encoding='utf-8') as f:\n",
    "                writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_MINIMAL)\n",
    "                writer.writeheader()\n",
    "                writer.writerows(rows)\n",
    "            \n",
    "            print(f\"âœ“ Exported {len(rows)} raw documents to {output_file}\")\n",
    "            print(f\"\\nFile structure:\")\n",
    "            print(f\"  - doc_id: Links to main export\")\n",
    "            print(f\"  - content: Full raw content\")\n",
    "            print(f\"  - doc_type: Type (text/table/image)\")\n",
    "            print(f\"  - source_file: Original PDF filename\")\n",
    "            print(f\"  - created_at: Export timestamp\")\n",
    "            \n",
    "            # Show content size statistics\n",
    "            total_size = sum(len(row['content']) for row in rows)\n",
    "            avg_size = total_size / len(rows) if rows else 0\n",
    "            print(f\"\\nğŸ“ Content Statistics:\")\n",
    "            print(f\"  Total characters: {total_size:,}\")\n",
    "            print(f\"  Average size: {avg_size:,.0f} chars\")\n",
    "            \n",
    "        else:\n",
    "            print(\"âš  No raw content found in docstore\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error exporting raw content: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "def export_complete_dataset(base_name: str = \"nasa_rag_export\"):\n",
    "    \"\"\"\n",
    "    Export both vector embeddings and raw content in one go.\n",
    "    Creates two CSV files that can be imported together.\n",
    "    \n",
    "    Args:\n",
    "        base_name: Base name for output files (will add _vectors.csv and _content.csv)\n",
    "    \"\"\"\n",
    "    print(\"ğŸ“¦ Exporting Complete Dataset\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Export vectors\n",
    "    vectors_file = f\"{base_name}_vectors.csv\"\n",
    "    export_chroma_to_csv(vectors_file)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Export raw content\n",
    "    content_file = f\"{base_name}_content.csv\"\n",
    "    export_raw_content_to_csv(content_file)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"âœ“ Export complete!\")\n",
    "    print(f\"\\nğŸ“ Files created:\")\n",
    "    print(f\"  1. {vectors_file} - Vector embeddings and summaries\")\n",
    "    print(f\"  2. {content_file} - Raw document content\")\n",
    "    print(f\"\\nğŸ’¡ Import instructions for backend team:\")\n",
    "    print(f\"  - Both files should be imported\")\n",
    "    print(f\"  - Link records using 'doc_id' field\")\n",
    "    print(f\"  - Parse 'embedding' JSON array for pgvector\")\n",
    "    print(f\"  - Parse 'metadata' JSON for additional fields\")\n",
    "    print(f\"\\nğŸ“‹ Example Python import code:\")\n",
    "    print(\"\"\"\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Read the CSVs\n",
    "    vectors_df = pd.read_csv('nasa_rag_export_vectors.csv')\n",
    "    content_df = pd.read_csv('nasa_rag_export_content.csv')\n",
    "    \n",
    "    # Parse embeddings\n",
    "    vectors_df['embedding'] = vectors_df['embedding'].apply(json.loads)\n",
    "    vectors_df['metadata'] = vectors_df['metadata'].apply(json.loads)\n",
    "    \"\"\")\n",
    "\n",
    "# Example usage\n",
    "if texts or tables or images:\n",
    "    print(\"Export Functions Available:\\n\")\n",
    "    print(\"1. Export vector embeddings only:\")\n",
    "    print('   export_chroma_to_csv(\"my_export.csv\")\\n')\n",
    "    print(\"2. Export raw content only:\")\n",
    "    print('   export_raw_content_to_csv(\"raw_content.csv\")\\n')\n",
    "    print(\"3. Export complete dataset (recommended):\")\n",
    "    print('   export_complete_dataset(\"nasa_rag_export\")\\n')\n",
    "else:\n",
    "    print(\"Process documents first before exporting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95a4de14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Exporting Complete Dataset\n",
      "================================================================================\n",
      "\n",
      "Exporting ChromaDB to nasa_rag_export_vectors.csv...\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Exported 700 documents to nasa_rag_export_vectors.csv\n",
      "âœ“ Embedding dimension: 768\n",
      "\n",
      "File structure:\n",
      "  - id: Unique document identifier\n",
      "  - content: Document summary text\n",
      "  - embedding: Vector embedding (JSON array of floats)\n",
      "  - metadata: Full metadata (JSON)\n",
      "  - doc_type: Type (text/table/image)\n",
      "  - source_file: Original PDF filename\n",
      "  - doc_id: Link to raw content\n",
      "  - created_at: Export timestamp\n",
      "\n",
      "ğŸ“Š Export Statistics:\n",
      "  Total documents: 700\n",
      "  By type: {'text': 365, 'table': 70, 'image': 265}\n",
      "  By source: {'unknown': 74, 'paper1.pdf': 90, 'paper2.pdf': 111, 'PMC11500582.pdf': 16, 'PMC11988870.pdf': 23, 'PMC3040128.pdf': 58, 'PMC3177255.pdf': 25, 'PMC3630201.pdf': 30, 'PMC4095884.pdf': 24, 'PMC4136787.pdf': 37, 'PMC5387210.pdf': 23, 'PMC5460236.pdf': 18, 'PMC5587110.pdf': 45, 'PMC5666799.pdf': 40, 'PMC6222041.pdf': 23, 'PMC6813909.pdf': 19, 'PMC7998608.pdf': 23, 'PMC8396460.pdf': 21}\n",
      "\n",
      "\n",
      "Exporting raw content to nasa_rag_export_content.csv...\n",
      "================================================================================\n",
      "âœ“ Exported 425 raw documents to nasa_rag_export_content.csv\n",
      "\n",
      "File structure:\n",
      "  - doc_id: Links to main export\n",
      "  - content: Full raw content\n",
      "  - doc_type: Type (text/table/image)\n",
      "  - source_file: Original PDF filename\n",
      "  - created_at: Export timestamp\n",
      "\n",
      "ğŸ“ Content Statistics:\n",
      "  Total characters: 13,051,216\n",
      "  Average size: 30,709 chars\n",
      "\n",
      "================================================================================\n",
      "âœ“ Export complete!\n",
      "\n",
      "ğŸ“ Files created:\n",
      "  1. nasa_rag_export_vectors.csv - Vector embeddings and summaries\n",
      "  2. nasa_rag_export_content.csv - Raw document content\n",
      "\n",
      "ğŸ’¡ Import instructions for backend team:\n",
      "  - Both files should be imported\n",
      "  - Link records using 'doc_id' field\n",
      "  - Parse 'embedding' JSON array for pgvector\n",
      "  - Parse 'metadata' JSON for additional fields\n",
      "\n",
      "ğŸ“‹ Example Python import code:\n",
      "\n",
      "    import json\n",
      "    import pandas as pd\n",
      "    \n",
      "    # Read the CSVs\n",
      "    vectors_df = pd.read_csv('nasa_rag_export_vectors.csv')\n",
      "    content_df = pd.read_csv('nasa_rag_export_content.csv')\n",
      "    \n",
      "    # Parse embeddings\n",
      "    vectors_df['embedding'] = vectors_df['embedding'].apply(json.loads)\n",
      "    vectors_df['metadata'] = vectors_df['metadata'].apply(json.loads)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Export the complete dataset\n",
    "if texts or tables or images:\n",
    "    export_complete_dataset(\"nasa_rag_export\")\n",
    "else:\n",
    "    print(\"No documents to export. Process PDFs first.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
